{"ast":null,"code":"var _jsxFileName = \"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\ai-interview-model\\\\client\\\\src\\\\InterviewPage.js\",\n  _s = $RefreshSig$();\nimport React, { useState, useEffect, useCallback, useRef } from 'react';\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nconst InterviewPage = ({\n  onComplete\n}) => {\n  _s();\n  const [transcript, setTranscript] = useState('');\n  const [isListening, setIsListening] = useState(false);\n  const [aiText, setAiText] = useState(\"Hello! Let's begin the interview. Could you please tell me a bit about yourself?\");\n  const [interviewHistory, setInterviewHistory] = useState([]);\n  const [questionCount, setQuestionCount] = useState(0);\n\n  // Use useRef to create SpeechRecognition and SpeechSynthesis objects once\n  const recognitionRef = useRef(null);\n  const synthRef = useRef(null);\n  useEffect(() => {\n    // Initialize Web Speech API for speech-to-text\n    recognitionRef.current = new (window.SpeechRecognition || window.webkitSpeechRecognition)();\n    recognitionRef.current.continuous = false;\n    recognitionRef.current.interimResults = false;\n    recognitionRef.current.lang = 'en-US';\n\n    // Initialize Web Speech API for text-to-speech\n    synthRef.current = window.speechSynthesis;\n\n    // Cleanup function to stop recognition when the component unmounts\n    return () => {\n      if (recognitionRef.current) {\n        recognitionRef.current.stop();\n      }\n    };\n  }, []); // Empty dependency array ensures this runs only once\n\n  const sendToAI = useCallback(async text => {\n    // ... (Your existing sendToAI logic is fine)\n    const updatedHistory = [...interviewHistory, {\n      role: 'user',\n      text\n    }];\n    setInterviewHistory(updatedHistory);\n    try {\n      const response = await fetch('http://localhost:3001/api/chat', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({\n          chatHistory: updatedHistory,\n          userText: text\n        })\n      });\n      const data = await response.json();\n      setAiText(data.message);\n      setQuestionCount(prevCount => prevCount + 1);\n    } catch (error) {\n      console.error('Error fetching AI response:', error);\n      setAiText(\"I am sorry, there was an error. Could you please try again?\");\n    }\n  }, [interviewHistory]);\n  const speakAndListen = useCallback(text => {\n    // This is the core logic: speak first, then listen\n    setInterviewHistory(prevHistory => [...prevHistory, {\n      role: 'ai',\n      text: text\n    }]);\n    const utterance = new SpeechSynthesisUtterance(text);\n    utterance.onend = () => {\n      // After AI finishes speaking, start listening\n      if (questionCount < 5) {\n        if (recognitionRef.current) {\n          setIsListening(true);\n          recognitionRef.current.start();\n        }\n      } else {\n        // Interview completed\n        onComplete([...interviewHistory, {\n          role: 'ai',\n          text: aiText\n        }]);\n      }\n    };\n    synthRef.current.speak(utterance);\n  }, [onComplete, interviewHistory, questionCount, aiText]);\n\n  // Effect to handle the initial greeting and the main interview loop\n  useEffect(() => {\n    if (aiText) {\n      speakAndListen(aiText);\n    }\n  }, [aiText, speakAndListen]);\n\n  // Effect for setting up the event listeners on the recognition object\n  // This is where we handle user input and errors\n  useEffect(() => {\n    if (recognitionRef.current) {\n      recognitionRef.current.onresult = event => {\n        const last = event.results.length - 1;\n        const text = event.results[last][0].transcript;\n        setTranscript(text);\n        setIsListening(false);\n        sendToAI(text);\n      };\n      recognitionRef.current.onerror = event => {\n        console.error('Speech recognition error:', event.error);\n        setIsListening(false);\n        // Retry listening after an error, if the interview is not complete\n        if (questionCount < 5) {\n          if (recognitionRef.current) {\n            recognitionRef.current.start();\n          }\n        }\n      };\n\n      // Crucial: The `onend` event listener\n      recognitionRef.current.onend = () => {\n        setIsListening(false);\n        console.log('Recognition session ended.');\n        // If the interview is not over and we're not currently speaking,\n        // you could restart listening here if needed for more continuous interaction.\n        // For this specific flow, the `speakAndListen` function handles the next step.\n      };\n    }\n  }, [sendToAI, questionCount]);\n\n  // Handle the very first question when the component mounts\n  useEffect(() => {\n    // This ensures the first question is asked only once\n    if (questionCount === 0 && aiText) {\n      speakAndListen(aiText);\n    }\n  }, [questionCount, aiText, speakAndListen]);\n  return /*#__PURE__*/_jsxDEV(\"div\", {\n    children: [/*#__PURE__*/_jsxDEV(\"p\", {\n      children: [\"AI: \", aiText]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 127,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"p\", {\n      children: [\"You: \", transcript || (isListening ? \"Listening...\" : \"Waiting for your response...\")]\n    }, void 0, true, {\n      fileName: _jsxFileName,\n      lineNumber: 128,\n      columnNumber: 7\n    }, this), /*#__PURE__*/_jsxDEV(\"div\", {\n      children: /*#__PURE__*/_jsxDEV(\"button\", {\n        onClick: () => {\n          // Manual start button: good for debugging\n          if (!isListening && recognitionRef.current) {\n            setIsListening(true);\n            recognitionRef.current.start();\n          }\n        },\n        disabled: isListening,\n        children: isListening ? \"Listening...\" : \"Speak now\"\n      }, void 0, false, {\n        fileName: _jsxFileName,\n        lineNumber: 130,\n        columnNumber: 9\n      }, this)\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 129,\n      columnNumber: 7\n    }, this)]\n  }, void 0, true, {\n    fileName: _jsxFileName,\n    lineNumber: 126,\n    columnNumber: 5\n  }, this);\n};\n_s(InterviewPage, \"iMhsgGTyoN+IAik/rzAfqu/eqeE=\");\n_c = InterviewPage;\nexport default InterviewPage;\nvar _c;\n$RefreshReg$(_c, \"InterviewPage\");","map":{"version":3,"names":["React","useState","useEffect","useCallback","useRef","jsxDEV","_jsxDEV","InterviewPage","onComplete","_s","transcript","setTranscript","isListening","setIsListening","aiText","setAiText","interviewHistory","setInterviewHistory","questionCount","setQuestionCount","recognitionRef","synthRef","current","window","SpeechRecognition","webkitSpeechRecognition","continuous","interimResults","lang","speechSynthesis","stop","sendToAI","text","updatedHistory","role","response","fetch","method","headers","body","JSON","stringify","chatHistory","userText","data","json","message","prevCount","error","console","speakAndListen","prevHistory","utterance","SpeechSynthesisUtterance","onend","start","speak","onresult","event","last","results","length","onerror","log","children","fileName","_jsxFileName","lineNumber","columnNumber","onClick","disabled","_c","$RefreshReg$"],"sources":["C:/Users/Lenovo/Desktop/ai-interview-model/client/src/InterviewPage.js"],"sourcesContent":["import React, { useState, useEffect, useCallback, useRef } from 'react';\r\n\r\nconst InterviewPage = ({ onComplete }) => {\r\n  const [transcript, setTranscript] = useState('');\r\n  const [isListening, setIsListening] = useState(false);\r\n  const [aiText, setAiText] = useState(\"Hello! Let's begin the interview. Could you please tell me a bit about yourself?\");\r\n  const [interviewHistory, setInterviewHistory] = useState([]);\r\n  const [questionCount, setQuestionCount] = useState(0);\r\n\r\n  // Use useRef to create SpeechRecognition and SpeechSynthesis objects once\r\n  const recognitionRef = useRef(null);\r\n  const synthRef = useRef(null);\r\n\r\n  useEffect(() => {\r\n    // Initialize Web Speech API for speech-to-text\r\n    recognitionRef.current = new (window.SpeechRecognition || window.webkitSpeechRecognition)();\r\n    recognitionRef.current.continuous = false;\r\n    recognitionRef.current.interimResults = false;\r\n    recognitionRef.current.lang = 'en-US';\r\n\r\n    // Initialize Web Speech API for text-to-speech\r\n    synthRef.current = window.speechSynthesis;\r\n    \r\n    // Cleanup function to stop recognition when the component unmounts\r\n    return () => {\r\n      if (recognitionRef.current) {\r\n        recognitionRef.current.stop();\r\n      }\r\n    };\r\n  }, []); // Empty dependency array ensures this runs only once\r\n\r\n  const sendToAI = useCallback(async (text) => {\r\n    // ... (Your existing sendToAI logic is fine)\r\n    const updatedHistory = [...interviewHistory, { role: 'user', text }];\r\n    setInterviewHistory(updatedHistory);\r\n    \r\n    try {\r\n      const response = await fetch('http://localhost:3001/api/chat', {\r\n        method: 'POST',\r\n        headers: { 'Content-Type': 'application/json' },\r\n        body: JSON.stringify({ chatHistory: updatedHistory, userText: text }),\r\n      });\r\n      const data = await response.json();\r\n      setAiText(data.message);\r\n      setQuestionCount(prevCount => prevCount + 1);\r\n    } catch (error) {\r\n      console.error('Error fetching AI response:', error);\r\n      setAiText(\"I am sorry, there was an error. Could you please try again?\");\r\n    }\r\n  }, [interviewHistory]);\r\n\r\n  const speakAndListen = useCallback((text) => {\r\n    // This is the core logic: speak first, then listen\r\n    setInterviewHistory(prevHistory => [...prevHistory, { role: 'ai', text: text }]);\r\n    const utterance = new SpeechSynthesisUtterance(text);\r\n    \r\n    utterance.onend = () => {\r\n      // After AI finishes speaking, start listening\r\n      if (questionCount < 5) {\r\n        if (recognitionRef.current) {\r\n          setIsListening(true);\r\n          recognitionRef.current.start();\r\n        }\r\n      } else {\r\n        // Interview completed\r\n        onComplete([...interviewHistory, { role: 'ai', text: aiText }]);\r\n      }\r\n    };\r\n    \r\n    synthRef.current.speak(utterance);\r\n  }, [onComplete, interviewHistory, questionCount, aiText]);\r\n\r\n\r\n  // Effect to handle the initial greeting and the main interview loop\r\n  useEffect(() => {\r\n    if (aiText) {\r\n      speakAndListen(aiText);\r\n    }\r\n  }, [aiText, speakAndListen]);\r\n\r\n\r\n  // Effect for setting up the event listeners on the recognition object\r\n  // This is where we handle user input and errors\r\n  useEffect(() => {\r\n    if (recognitionRef.current) {\r\n      recognitionRef.current.onresult = (event) => {\r\n        const last = event.results.length - 1;\r\n        const text = event.results[last][0].transcript;\r\n        setTranscript(text);\r\n        setIsListening(false);\r\n        sendToAI(text);\r\n      };\r\n\r\n      recognitionRef.current.onerror = (event) => {\r\n        console.error('Speech recognition error:', event.error);\r\n        setIsListening(false);\r\n        // Retry listening after an error, if the interview is not complete\r\n        if (questionCount < 5) {\r\n          if (recognitionRef.current) {\r\n            recognitionRef.current.start();\r\n          }\r\n        }\r\n      };\r\n\r\n      // Crucial: The `onend` event listener\r\n      recognitionRef.current.onend = () => {\r\n        setIsListening(false);\r\n        console.log('Recognition session ended.');\r\n        // If the interview is not over and we're not currently speaking,\r\n        // you could restart listening here if needed for more continuous interaction.\r\n        // For this specific flow, the `speakAndListen` function handles the next step.\r\n      };\r\n    }\r\n  }, [sendToAI, questionCount]);\r\n\r\n  // Handle the very first question when the component mounts\r\n  useEffect(() => {\r\n    // This ensures the first question is asked only once\r\n    if (questionCount === 0 && aiText) {\r\n        speakAndListen(aiText);\r\n    }\r\n  }, [questionCount, aiText, speakAndListen]);\r\n\r\n\r\n  return (\r\n    <div>\r\n      <p>AI: {aiText}</p>\r\n      <p>You: {transcript || (isListening ? \"Listening...\" : \"Waiting for your response...\")}</p>\r\n      <div>\r\n        <button onClick={() => {\r\n            // Manual start button: good for debugging\r\n            if (!isListening && recognitionRef.current) {\r\n                setIsListening(true);\r\n                recognitionRef.current.start();\r\n            }\r\n        }} disabled={isListening}>\r\n          {isListening ? \"Listening...\" : \"Speak now\"}\r\n        </button>\r\n      </div>\r\n    </div>\r\n  );\r\n};\r\n\r\nexport default InterviewPage;"],"mappings":";;AAAA,OAAOA,KAAK,IAAIC,QAAQ,EAAEC,SAAS,EAAEC,WAAW,EAAEC,MAAM,QAAQ,OAAO;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAExE,MAAMC,aAAa,GAAGA,CAAC;EAAEC;AAAW,CAAC,KAAK;EAAAC,EAAA;EACxC,MAAM,CAACC,UAAU,EAAEC,aAAa,CAAC,GAAGV,QAAQ,CAAC,EAAE,CAAC;EAChD,MAAM,CAACW,WAAW,EAAEC,cAAc,CAAC,GAAGZ,QAAQ,CAAC,KAAK,CAAC;EACrD,MAAM,CAACa,MAAM,EAAEC,SAAS,CAAC,GAAGd,QAAQ,CAAC,kFAAkF,CAAC;EACxH,MAAM,CAACe,gBAAgB,EAAEC,mBAAmB,CAAC,GAAGhB,QAAQ,CAAC,EAAE,CAAC;EAC5D,MAAM,CAACiB,aAAa,EAAEC,gBAAgB,CAAC,GAAGlB,QAAQ,CAAC,CAAC,CAAC;;EAErD;EACA,MAAMmB,cAAc,GAAGhB,MAAM,CAAC,IAAI,CAAC;EACnC,MAAMiB,QAAQ,GAAGjB,MAAM,CAAC,IAAI,CAAC;EAE7BF,SAAS,CAAC,MAAM;IACd;IACAkB,cAAc,CAACE,OAAO,GAAG,KAAKC,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB,EAAE,CAAC;IAC3FL,cAAc,CAACE,OAAO,CAACI,UAAU,GAAG,KAAK;IACzCN,cAAc,CAACE,OAAO,CAACK,cAAc,GAAG,KAAK;IAC7CP,cAAc,CAACE,OAAO,CAACM,IAAI,GAAG,OAAO;;IAErC;IACAP,QAAQ,CAACC,OAAO,GAAGC,MAAM,CAACM,eAAe;;IAEzC;IACA,OAAO,MAAM;MACX,IAAIT,cAAc,CAACE,OAAO,EAAE;QAC1BF,cAAc,CAACE,OAAO,CAACQ,IAAI,CAAC,CAAC;MAC/B;IACF,CAAC;EACH,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC;;EAER,MAAMC,QAAQ,GAAG5B,WAAW,CAAC,MAAO6B,IAAI,IAAK;IAC3C;IACA,MAAMC,cAAc,GAAG,CAAC,GAAGjB,gBAAgB,EAAE;MAAEkB,IAAI,EAAE,MAAM;MAAEF;IAAK,CAAC,CAAC;IACpEf,mBAAmB,CAACgB,cAAc,CAAC;IAEnC,IAAI;MACF,MAAME,QAAQ,GAAG,MAAMC,KAAK,CAAC,gCAAgC,EAAE;QAC7DC,MAAM,EAAE,MAAM;QACdC,OAAO,EAAE;UAAE,cAAc,EAAE;QAAmB,CAAC;QAC/CC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UAAEC,WAAW,EAAET,cAAc;UAAEU,QAAQ,EAAEX;QAAK,CAAC;MACtE,CAAC,CAAC;MACF,MAAMY,IAAI,GAAG,MAAMT,QAAQ,CAACU,IAAI,CAAC,CAAC;MAClC9B,SAAS,CAAC6B,IAAI,CAACE,OAAO,CAAC;MACvB3B,gBAAgB,CAAC4B,SAAS,IAAIA,SAAS,GAAG,CAAC,CAAC;IAC9C,CAAC,CAAC,OAAOC,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,6BAA6B,EAAEA,KAAK,CAAC;MACnDjC,SAAS,CAAC,6DAA6D,CAAC;IAC1E;EACF,CAAC,EAAE,CAACC,gBAAgB,CAAC,CAAC;EAEtB,MAAMkC,cAAc,GAAG/C,WAAW,CAAE6B,IAAI,IAAK;IAC3C;IACAf,mBAAmB,CAACkC,WAAW,IAAI,CAAC,GAAGA,WAAW,EAAE;MAAEjB,IAAI,EAAE,IAAI;MAAEF,IAAI,EAAEA;IAAK,CAAC,CAAC,CAAC;IAChF,MAAMoB,SAAS,GAAG,IAAIC,wBAAwB,CAACrB,IAAI,CAAC;IAEpDoB,SAAS,CAACE,KAAK,GAAG,MAAM;MACtB;MACA,IAAIpC,aAAa,GAAG,CAAC,EAAE;QACrB,IAAIE,cAAc,CAACE,OAAO,EAAE;UAC1BT,cAAc,CAAC,IAAI,CAAC;UACpBO,cAAc,CAACE,OAAO,CAACiC,KAAK,CAAC,CAAC;QAChC;MACF,CAAC,MAAM;QACL;QACA/C,UAAU,CAAC,CAAC,GAAGQ,gBAAgB,EAAE;UAAEkB,IAAI,EAAE,IAAI;UAAEF,IAAI,EAAElB;QAAO,CAAC,CAAC,CAAC;MACjE;IACF,CAAC;IAEDO,QAAQ,CAACC,OAAO,CAACkC,KAAK,CAACJ,SAAS,CAAC;EACnC,CAAC,EAAE,CAAC5C,UAAU,EAAEQ,gBAAgB,EAAEE,aAAa,EAAEJ,MAAM,CAAC,CAAC;;EAGzD;EACAZ,SAAS,CAAC,MAAM;IACd,IAAIY,MAAM,EAAE;MACVoC,cAAc,CAACpC,MAAM,CAAC;IACxB;EACF,CAAC,EAAE,CAACA,MAAM,EAAEoC,cAAc,CAAC,CAAC;;EAG5B;EACA;EACAhD,SAAS,CAAC,MAAM;IACd,IAAIkB,cAAc,CAACE,OAAO,EAAE;MAC1BF,cAAc,CAACE,OAAO,CAACmC,QAAQ,GAAIC,KAAK,IAAK;QAC3C,MAAMC,IAAI,GAAGD,KAAK,CAACE,OAAO,CAACC,MAAM,GAAG,CAAC;QACrC,MAAM7B,IAAI,GAAG0B,KAAK,CAACE,OAAO,CAACD,IAAI,CAAC,CAAC,CAAC,CAAC,CAACjD,UAAU;QAC9CC,aAAa,CAACqB,IAAI,CAAC;QACnBnB,cAAc,CAAC,KAAK,CAAC;QACrBkB,QAAQ,CAACC,IAAI,CAAC;MAChB,CAAC;MAEDZ,cAAc,CAACE,OAAO,CAACwC,OAAO,GAAIJ,KAAK,IAAK;QAC1CT,OAAO,CAACD,KAAK,CAAC,2BAA2B,EAAEU,KAAK,CAACV,KAAK,CAAC;QACvDnC,cAAc,CAAC,KAAK,CAAC;QACrB;QACA,IAAIK,aAAa,GAAG,CAAC,EAAE;UACrB,IAAIE,cAAc,CAACE,OAAO,EAAE;YAC1BF,cAAc,CAACE,OAAO,CAACiC,KAAK,CAAC,CAAC;UAChC;QACF;MACF,CAAC;;MAED;MACAnC,cAAc,CAACE,OAAO,CAACgC,KAAK,GAAG,MAAM;QACnCzC,cAAc,CAAC,KAAK,CAAC;QACrBoC,OAAO,CAACc,GAAG,CAAC,4BAA4B,CAAC;QACzC;QACA;QACA;MACF,CAAC;IACH;EACF,CAAC,EAAE,CAAChC,QAAQ,EAAEb,aAAa,CAAC,CAAC;;EAE7B;EACAhB,SAAS,CAAC,MAAM;IACd;IACA,IAAIgB,aAAa,KAAK,CAAC,IAAIJ,MAAM,EAAE;MAC/BoC,cAAc,CAACpC,MAAM,CAAC;IAC1B;EACF,CAAC,EAAE,CAACI,aAAa,EAAEJ,MAAM,EAAEoC,cAAc,CAAC,CAAC;EAG3C,oBACE5C,OAAA;IAAA0D,QAAA,gBACE1D,OAAA;MAAA0D,QAAA,GAAG,MAAI,EAAClD,MAAM;IAAA;MAAAmD,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAI,CAAC,eACnB9D,OAAA;MAAA0D,QAAA,GAAG,OAAK,EAACtD,UAAU,KAAKE,WAAW,GAAG,cAAc,GAAG,8BAA8B,CAAC;IAAA;MAAAqD,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OAAI,CAAC,eAC3F9D,OAAA;MAAA0D,QAAA,eACE1D,OAAA;QAAQ+D,OAAO,EAAEA,CAAA,KAAM;UACnB;UACA,IAAI,CAACzD,WAAW,IAAIQ,cAAc,CAACE,OAAO,EAAE;YACxCT,cAAc,CAAC,IAAI,CAAC;YACpBO,cAAc,CAACE,OAAO,CAACiC,KAAK,CAAC,CAAC;UAClC;QACJ,CAAE;QAACe,QAAQ,EAAE1D,WAAY;QAAAoD,QAAA,EACtBpD,WAAW,GAAG,cAAc,GAAG;MAAW;QAAAqD,QAAA,EAAAC,YAAA;QAAAC,UAAA;QAAAC,YAAA;MAAA,OACrC;IAAC;MAAAH,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,OACN,CAAC;EAAA;IAAAH,QAAA,EAAAC,YAAA;IAAAC,UAAA;IAAAC,YAAA;EAAA,OACH,CAAC;AAEV,CAAC;AAAC3D,EAAA,CA3IIF,aAAa;AAAAgE,EAAA,GAAbhE,aAAa;AA6InB,eAAeA,aAAa;AAAC,IAAAgE,EAAA;AAAAC,YAAA,CAAAD,EAAA","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}